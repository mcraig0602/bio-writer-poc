# Backend Environment Variables

# Server Configuration
PORT=3001

# MongoDB Configuration
MONGODB_URI=mongodb://mongodb:27017/biography-poc

# AI Configuration (OpenAI-style, preferred)
# These variables are designed to be provider-agnostic and OpenAI-compatible.
# For now, the backend still talks to Ollama, but AI_* overrides OLLAMA_*.
#
# Base URL for the AI provider (for Ollama in Docker Desktop, host.docker.internal works).
# In production containers, set this to the reachable host/service URL.
# AI_BASE_URL=http://host.docker.internal:11434
#
# Model name (maps to Ollama "model" today; future OpenAI-compatible models later)
# AI_MODEL=llama3.1
#
# API key (unused for Ollama today, kept for future OpenAI-compatible providers)
# AI_API_KEY=
#
# OpenAI-like generation knobs (optional; if unset, Ollama defaults apply)
# AI_TEMPERATURE=0.7
# AI_MAX_TOKENS=512
#
# HTTP + reliability knobs (optional)
# AI_TIMEOUT_MS=60000
# AI_MAX_RETRIES=2
# AI_RETRY_INITIAL_DELAY_MS=250
# AI_RETRY_BACKOFF_MULTIPLIER=2

# Ollama Configuration
OLLAMA_URL=http://host.docker.internal:11434
OLLAMA_MODEL=llama3.1

# Tracing Configuration
TRACING_ENABLED=true
TRACE_OUTPUT_DIR=./traces
TRACE_LEVEL=info
TRACE_PERSIST_TO_DB=true
TRACE_INCLUDE_PAYLOADS=true
TRACE_MAX_IN_MEMORY=1000
TRACE_SAMPLE_RATE=1.0

# Evaluation Configuration
EVALUATION_ENABLED=true
EVALUATION_BASELINE_DIR=./evaluation/baselines
EVALUATION_TEST_CASES=./src/evaluation/test-cases.json
EVALUATION_AUTO_BASELINE=false

# Agent Configuration
AGENT_MAX_RETRIES=3
AGENT_RETRY_DELAY_MS=1000
AGENT_RETRY_BACKOFF_MULTIPLIER=2
AGENT_MAX_CONTEXT_MESSAGES=20
AGENT_CONTEXT_SUMMARIZE_THRESHOLD=15
AGENT_PROMPT_VERSION=v1

# Logging Configuration
LOG_LEVEL=info
LOG_DIR=./logs
LOG_MAX_FILES=7d
LOG_MAX_SIZE=20m
LOG_REQUESTS=true              # Enable/disable HTTP request logging
LOG_REQUEST_BODY=false         # Log request bodies (off by default for security)

# TODO: Add authorization configuration for multi-user support
# AUTH_SECRET=your-secret-key
# AUTH_ISSUER=biography-poc
